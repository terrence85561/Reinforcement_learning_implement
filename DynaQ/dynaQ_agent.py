"""
Sarsa agent for windy gridworld problem.
"""

from rl_glue import BaseAgent
import numpy as np


class DynaQAgent(BaseAgent):
    """
    SarsaAgent -- Section 6.4 from RL book (2nd edition)
    """

    def __init__(self,n_ite):
        """Declare agent variables."""
        # init planning times
        self.n = n_ite
    def agent_init(self):
        """
        Arguments: Nothing
        Returns: Nothing
        Hint: Initialize the variables that need to be reset before each run
        begins
        """
        
        # init epsilon
        self.epsilon = 0.1
        # init alpha
        self.alpha = 0.1
        # init gamma
        self.gamma = 0.95

        # init an action dictionary, where key is the action's number
        # and the value is how will the action change the coordinate of state
        self.action_dict = {
            0: [-1, 0],    # go up
            1: [1, 0],   # go down
            2: [0, -1],   # go left
            3: [0, 1],    # go right
        }

        # init number of girdworld's row and col
        self.maxRow = 7
        self.maxCol = 10

        # init Q, action value for all state action pairs
        # self.Q = {}
        # for row in range(self.maxRow):
        #     for col in range(self.maxCol):
        #         self.Q[(row, col)] = np.zeros(len(self.action_dict))
        self.Q = np.zeros((self.maxRow,self.maxCol,len(self.action_dict)))
        
        # init Model, the key is state tuple, value is array in length of 4
        self.model = {}
        for row in range(self.maxRow):
            for col in range(self.maxCol):
                self.model[(row,col)] = [0,0,0,0]
        # init an array to record last state and action

        # init an array to record observed states
        self.observed = []


        # init the action's sequence number in self.action_dict
        self.action = None

        # init actural movement, i.e the moves generated by self.action_dict
        self.move = None

        self.state = None

    def agent_start(self, state):
        
        self.record = [state]
        """
        Arguments: state - tuple
        Returns: action - integer
        grab the initial state, use epsilon greedy method
        to generate action
        """
        choice = np.random.choice([0, 1], p=[self.epsilon, 1-self.epsilon])
        if choice == 0:
            # exploring
            self.action = np.random.randint(0, len(self.action_dict))
        elif choice == 1:
            # exploiting
            #self.action = np.argmax(self.Q[state])
            self.action = self.my_argmax(self.Q[state])

        
        # get the actural movement
        move = self.action_dict[self.action]

        # record this state_action pair for the coming update
        #self.record = [state, self.action]
        self.state = state
        # record the state 
        if state not in self.observed:
            self.observed.append(state)
        #print(move)
        return move

    def agent_step(self, reward, state):
        
        #print('the action value function for (1,8)',self.Q[(2,8)])
        """
        Arguments: reward - floting point, state - tuple
        Returns: action - integer
        this reward is the reward for last state_action pair
        and state argument is the next state
        """
        #print('state is ',state)
        self.record.append(state)
        # epsilon greedy to generate corresponding action
        choice = np.random.choice([0, 1], p=[self.epsilon, 1-self.epsilon])
        if choice == 0:
            # exploring
            #self.action = np.random.randint(0, len(self.action_dict))
            action = np.random.randint(0, len(self.action_dict))

        else:
            #exploiting
            #action = np.argmax(self.Q[state])
            action = self.my_argmax(self.Q[state])

        # get the last state and action to update
        #last_state = self.record[0]
        # this is the sequence number of self.action_dict
        #last_action = self.record[1]

        # by Q-learning update rule

        self.Q[self.state[0]][self.state[1]][self.action] = self.Q[self.state[0]][self.state[1]][self.action] + self.alpha * (reward +
                                        self.gamma * np.max(self.Q[state[0]][state[1]]) - self.Q[self.state[0]][self.state[1]][self.action])

        # if last_state == (3,7):
        #     print(self.Q[last_state])
        # need to store the passed in reward and state to model
        self.model[self.state][self.action] = (reward,state)
        # do planning
        self.planning(self.observed,self.model)
        
        # record this state_action pair for the coming update
        #self.record = [state, self.action]

        # record the state as observed
        if state not in self.observed:
            self.observed.append(state)

        # get the actural move from self.action_dict
        move = self.action_dict[action]
        self.action = action
        self.state = state
        #print(move)
        return move

    def agent_end(self, reward):
        """
        Arguments: reward - floating point
        Returns: Nothing
        do the last update when reach the terminal state
        the action value for terminal state is zero
        """
        # get the last state and action to update
        #last_state = self.record[0]
        # this is the sequence number of self.action_dict
        #last_action = self.record[1]

        # by Sarsa update rule
        self.Q[self.state[0]][self.state[1]][self.action] = self.Q[self.state[0]][self.state[1]][self.action] + self.alpha * (reward +
                                                                                                                              self.gamma * 0 - self.Q[self.state[0]][self.state[1]][self.action])

        # need to store the passed in reward and state to model
        self.model[self.state][self.action] = (reward,'terminal')
        self.planning(self.observed,self.model)
        
    def agent_message(self, in_message):
        """
        Arguments: in_message - string
        Returns: The value function as a list.
        This function is complete. You do not need to add code here.
        """
        if in_message == 'Q for all states in the episode':

            # #return (np.max(self.Q, axis=1)).tostring()
            # print('Q(2,0) is {} Q(2,1) is {}, Q(3,1) is {}, Q(4,1) is {}, Q(4,2) is {} \n \
            #     Q(4,3) is {}, Q(3,3) is {}, Q(3,4) is {}, Q(3,5) is {}, Q(3,6) is {} \n \
            #     Q(3,7) is {}, Q(3,8) is {}, Q(2,8) is {}, Q(1,8) is {}'.format(self.Q[(2,0)],
            #     self.Q[(2,1)], self.Q[(3,1)],self.Q[(4,1)],self.Q[(4,2)],self.Q[(4,3)],self.Q[(3,3)],
            #     self.Q[(3,4)],self.Q[(3,5)],self.Q[(3,6)],self.Q[(3,7)],self.Q[(3,8)],self.Q[2,8],self.Q[1,8]))
            #return self.Q[in_message]
            return self.Q
        else:
            return "I dont know how to respond to this message!!"

    def planning (self,observed,model):
        
        #print('observed state',observed)
        for _ in range(self.n):
            
            idx = np.random.randint(0,len(observed))
            state = observed[idx]
            if state == 'terminal':
                print('terminal!')
                self.Q[state] = np.ones(4)
            # bad idea, need to be modified
            else:
                arr = []
                action_list = model[state]
                for i in range(len(action_list)):
                    if action_list[i] != 0:
                        #print("arr",action_list[i])
                        arr.append(i)

                # randomly choose an action sequence number
                action = np.random.choice(arr)
                #print('selected state is ',state, 'selected action is', action)
                reward,nextstate = model[state][action]
                if nextstate == 'terminal':
                    # Q-learning update
                    self.Q[state][action] = self.Q[state][action] + \
                        self.alpha * (reward + self.gamma * 0 -
                                  self.Q[state][action])
                else:
                    # Q-learning update
                    self.Q[state][action] = self.Q[state][action] + \
                            self.alpha * (reward +self.gamma * np.max(self.Q[nextstate]) - \
                            self.Q[state][action])
            
    def my_argmax(self,arr):
        # the input arr is the array of action value function for each action in a certain state
        # my_argmax is a tie breaker function for argmax
        max_arg = []
        max_value = np.max(arr)
        for i in range(len(arr)):
            if arr[i] == max_value:
                max_arg.append(i)
        if len(arr) > 1:
            return np.random.choice(max_arg)
        else:
            return arr[0]

                