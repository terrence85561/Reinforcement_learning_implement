"""
Sarsa agent for windy gridworld problem.
"""

from rl_glue import BaseAgent
import numpy as np

class SarsaAgent(BaseAgent):
    """
    SarsaAgent -- Section 6.4 from RL book (2nd edition)
    """

    def __init__(self):
        """Declare agent variables."""


    def agent_init(self):
        """
        Arguments: Nothing
        Returns: Nothing
        Hint: Initialize the variables that need to be reset before each run
        begins
        """
        # init epsilon
        self.epsilon = 0.1
        # init alpha
        self.alpha = 0.5
        # init gamma
        self.gamma = 1

        # init an action dictionary, where key is the action's number
        # and the value is how will the action change the coordinate of state
        # have to add one more move if we change to 9 moves
        self.action_dict = {
        0:[-1,0],    # go north
        1:[1,0],   # go south
        2:[0,-1],   # go west
        3:[0,1],    # go east
        4:[-1,1],    # go northeast
        5:[1,1],   # go southeast
        6:[-1,-1],   # go northwest
        7:[1,-1],   # go southwest
        #8:[0,0] # stay
        }

        # init number of girdworld's row and col
        self.maxRow = 7
        self.maxCol = 10

        # init Q, action value for all state action pairs
        self.Q = {}
        for row in range (self.maxRow):
            for col in range(self.maxCol):
                self.Q[(row,col)] = np.zeros(len(self.action_dict))

        # init an array to record last state and action
        self.record = None

        # init the action's sequence number in self.action_dict
        self.action = None

        # init actural movement, i.e the moves generated by self.action_dict
        self.move = None

    def agent_start(self,state):
        """
        Arguments: state - tuple
        Returns: action - integer
        grab the initial state, use epsilon greedy method
        to generate action
        """
        choice = np.random.choice([0,1],p = [self.epsilon,1-self.epsilon])
        if choice == 0:
            # exploring
            self.action = np.random.randint(0,len(self.action_dict))
        else:
            # exploiting
            self.action = np.argmax(self.Q[state])
            #self.action = self.my_argmax(self.Q[state])
        # get the actural movement
        move = self.action_dict[self.action]

        # record this state_action pair for the coming update
        self.record = [state,self.action]

        return move

    def agent_step(self, reward, state):
        """
        Arguments: reward - floting point, state - tuple
        Returns: action - integer
        this reward is the reward for last state_action pair
        and state argument is the next state
        """
        # epsilon greedy to generate corresponding action
        choice = np.random.choice([0,1], p = [self.epsilon,1-self.epsilon])
        if choice == 0:
            # exploring
            self.action = np.random.randint(0,len(self.action_dict))
        else:
            #exploiting
            self.action = np.argmax(self.Q[state])
            #self.action = self.my_argmax(self.Q[state])
        # get the last state and action to update
        last_state = self.record[0]
        last_action = self.record[1] # this is the sequence number of self.action_dict

        # by Sarsa update rule
        self.Q[last_state][last_action] = self.Q[last_state][last_action] + self.alpha * (reward +
                        self.gamma * self.Q[state][self.action] - self.Q[last_state][last_action])

        # record this state_action pair for the coming update
        self.record = [state,self.action]

        # get the actural move from self.action_dict
        move = self.action_dict[self.action]

        return move



    def agent_end(self, reward):
        """
        Arguments: reward - floating point
        Returns: Nothing
        do the last update when reach the terminal state
        the action value for terminal state is zero
        """
        # get the last state and action to update
        last_state = self.record[0]
        last_action = self.record[1] # this is the sequence number of self.action_dict

        # by Sarsa update rule
        self.Q[last_state][last_action] = self.Q[last_state][last_action] + self.alpha * (reward +
                        self.gamma * 0 - self.Q[last_state][last_action])

    def agent_message(self, in_message):
        """
        Arguments: in_message - string
        Returns: The value function as a list.
        This function is complete. You do not need to add code here.
        """
        if in_message == 'ValueFunction':
            #return (np.max(self.Q, axis=1)).tostring()
            return
        else:
            return "I dont know how to respond to this message!!"

    # def my_argmax(self,arr):
    #     """
    #     self define argmax to break tie
    #     """
    #     max_arg = []
    #     cur_max = 0
    #     for i in range(len(arr)):
    #         if arr[i] > cur_max:
    #             max_arg.append(i)
    #             cur_max = arr[i]
    #         else:
    #             max_arg.append(cur_max)
    #     if len(max_arg) > 1:
    #         return np.random.choice(max_arg)
    #     if len(max_arg) == 1:
    #         return max_arg[0]
